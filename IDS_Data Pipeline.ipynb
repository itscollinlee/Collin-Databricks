{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ff134ef-71c7-4fa7-ad4c-9dd39f3c80dd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import os\n",
    "import traceback\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6fdd0d-e789-46ee-b1fd-e2ee7e14f35e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql('''SELECT current_catalog(), current_schema()'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040785eb-98b0-4b4f-984c-cf3357fe8ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(''' SELECT current_catalog(), current_schema()'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1432d4cb-624f-4866-937f-6137c6aed280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/Volumes/international_debt_statistics_data/default/ids_data/IDS_ALLCountries_Data_1.csv', header = True, inferSchema = True )\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3ba2af5-faf9-4169-ad3c-6aa095136a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_validator(df):\n",
    "    non_ascii_pattern = r'[^\\x00-\\x7F]'\n",
    "\n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            na_count = df.filter(df[column].isNaN()).count()\n",
    "            print(f'Number of NAs in {column} is {na_count}')\n",
    "        except:\n",
    "            null_count = df.filter(df[column].isNull()).count()\n",
    "            print(f'Number of NULLS in {column} is {null_count}')\n",
    "        finally:\n",
    "            non_ascii_count = df.filter(df[column].rlike(non_ascii_pattern)).count()\n",
    "            print(f'Number of rows with non-ASCII characters in {column} is {non_ascii_count} count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725ea870-dcf0-46f9-bbb1-9626bdb3ac8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_validator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41774a31-5e3d-45f7-b599-49f6465377cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/Volumes/international_debt_statistics_data/default/ids_data/Country-Series - Metadata.csv', header = True, inferSchema = True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9e5bfd4-a889-4b60-904f-8d0b3eab3c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_validator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b4dcea-33d9-4402-a72b-69555e823621",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/Volumes/international_debt_statistics_data/default/ids_data/IDS_CountryMetaData.csv', header = True, inferSchema = True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f7c977-aaad-4115-a5c2-e53182cc9a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_validator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51f45289-3225-47e5-9eea-e0c1de3dfad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/Volumes/international_debt_statistics_data/default/ids_data/IDS_FootNoteMetaData.csv', header = True, inferSchema = True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617a8f6f-91bf-475f-8dca-1ba224aacb96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_validator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bf407a4-13d4-4ebb-8e6a-bb7faee54703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/Volumes/international_debt_statistics_data/default/ids_data/IDS_SeriesMetaData.csv', header = True, inferSchema = True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25fde597-668e-4f24-8399-c4a2b8dfd2e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_validator(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "881f3c1e-9972-43a9-a35c-d5db63201a69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('/Volumes/international_debt_statistics_data/default/ids_data/IDS_ALLCountries_Data_13.csv', header = True, inferSchema = True)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a37682a1-fc34-4a33-8182-4868140cdb40",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "    #Explicitly define schema for bronze level table\n",
    "\n",
    "    bronze_ids_schema = StructType([\n",
    "        StructField('country_name', StringType(), True),\n",
    "        StructField('country_code', StringType(), True),\n",
    "        StructField('counterpart_area_name', StringType(), True),\n",
    "        StructField('counterpart_area_code', StringType(), True),\n",
    "        StructField('series_name', StringType(), True),\n",
    "        StructField('series_code', StringType(), True),\n",
    "        StructField('1970', DoubleType(), True),\n",
    "        StructField('1971', DoubleType(), True),\n",
    "        StructField('1972', DoubleType(), True),\n",
    "        StructField('1973', DoubleType(), True),\n",
    "        StructField('1974', DoubleType(), True),\n",
    "        StructField('1975', DoubleType(), True),\n",
    "        StructField('1976', DoubleType(), True),\n",
    "        StructField('1977', DoubleType(), True),\n",
    "        StructField('1978', DoubleType(), True),\n",
    "        StructField('1979', DoubleType(), True),\n",
    "        StructField('1980', DoubleType(), True),\n",
    "        StructField('1981', DoubleType(), True),\n",
    "        StructField('1982', DoubleType(), True),\n",
    "        StructField('1983', DoubleType(), True),\n",
    "        StructField('1984', DoubleType(), True),\n",
    "        StructField('1985', DoubleType(), True),\n",
    "        StructField('1986', DoubleType(), True),\n",
    "        StructField('1987', DoubleType(), True),\n",
    "        StructField('1988', DoubleType(), True),\n",
    "        StructField('1989', DoubleType(), True),\n",
    "        StructField('1990', DoubleType(), True),\n",
    "        StructField('1991', DoubleType(), True),\n",
    "        StructField('1992', DoubleType(), True),\n",
    "        StructField('1993', DoubleType(), True),\n",
    "        StructField('1994', DoubleType(), True),\n",
    "        StructField('1995', DoubleType(), True),\n",
    "        StructField('1996', DoubleType(), True),\n",
    "        StructField('1997', DoubleType(), True),\n",
    "        StructField('1998', DoubleType(), True),\n",
    "        StructField('1999', DoubleType(), True),\n",
    "        StructField('2000', DoubleType(), True),\n",
    "        StructField('2001', DoubleType(), True),\n",
    "        StructField('2002', DoubleType(), True),\n",
    "        StructField('2003', DoubleType(), True),\n",
    "        StructField('2004', DoubleType(), True),\n",
    "        StructField('2005', DoubleType(), True),\n",
    "        StructField('2006', DoubleType(), True),\n",
    "        StructField('2007', DoubleType(), True),\n",
    "        StructField('2008', DoubleType(), True),\n",
    "        StructField('2009', DoubleType(), True),\n",
    "        StructField('2010', DoubleType(), True),\n",
    "        StructField('2011', DoubleType(), True),\n",
    "        StructField('2012', DoubleType(), True),\n",
    "        StructField('2013', DoubleType(), True),\n",
    "        StructField('2014', DoubleType(), True),\n",
    "        StructField('2015', DoubleType(), True),\n",
    "        StructField('2016', DoubleType(), True),\n",
    "        StructField('2017', DoubleType(), True  ),\n",
    "        StructField('2018', DoubleType(), True),\n",
    "        StructField('2019', DoubleType(), True),\n",
    "        StructField('2020', DoubleType(), True),\n",
    "        StructField('2021', DoubleType(), True),\n",
    "        StructField('2022', DoubleType(), True),\n",
    "        StructField('2023', DoubleType(), True),\n",
    "        StructField('2024', DoubleType(), True),\n",
    "        StructField('2025', DoubleType(), True),\n",
    "        StructField('2026', DoubleType(), True),\n",
    "        StructField('2027', DoubleType(), True),\n",
    "        StructField('2028', DoubleType(), True),\n",
    "        StructField('2029', DoubleType(), True),\n",
    "        StructField('2030', DoubleType(), True),\n",
    "        StructField('2031', DoubleType(), True),\n",
    "        StructField('2032', DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a429de2b-b0b1-4505-bbe7-9a9f7548014a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Explicitly define schema for silver level table\n",
    "\n",
    "silver_ids_schema = StructType([\n",
    "        StructField('country_name', StringType(), True),\n",
    "        StructField('country_code', StringType(), True),\n",
    "        StructField('counterpart_area_name', StringType(), True),\n",
    "        StructField('counterpart_area_code', StringType(), True),\n",
    "        StructField('series_name', StringType(), True),\n",
    "        StructField('series_code', StringType(), True),\n",
    "        StructField('1970', DoubleType(), True),\n",
    "        StructField('1971', DoubleType(), True),\n",
    "        StructField('1972', DoubleType(), True),\n",
    "        StructField('1973', DoubleType(), True),\n",
    "        StructField('1974', DoubleType(), True),\n",
    "        StructField('1975', DoubleType(), True),\n",
    "        StructField('1976', DoubleType(), True),\n",
    "        StructField('1977', DoubleType(), True),\n",
    "        StructField('1978', DoubleType(), True),\n",
    "        StructField('1979', DoubleType(), True),\n",
    "        StructField('1980', DoubleType(), True),\n",
    "        StructField('1981', DoubleType(), True),\n",
    "        StructField('1982', DoubleType(), True),\n",
    "        StructField('1983', DoubleType(), True),\n",
    "        StructField('1984', DoubleType(), True),\n",
    "        StructField('1985', DoubleType(), True),\n",
    "        StructField('1986', DoubleType(), True),\n",
    "        StructField('1987', DoubleType(), True),\n",
    "        StructField('1988', DoubleType(), True),\n",
    "        StructField('1989', DoubleType(), True),\n",
    "        StructField('1990', DoubleType(), True),\n",
    "        StructField('1991', DoubleType(), True),\n",
    "        StructField('1992', DoubleType(), True),\n",
    "        StructField('1993', DoubleType(), True),\n",
    "        StructField('1994', DoubleType(), True),\n",
    "        StructField('1995', DoubleType(), True),\n",
    "        StructField('1996', DoubleType(), True),\n",
    "        StructField('1997', DoubleType(), True),\n",
    "        StructField('1998', DoubleType(), True),\n",
    "        StructField('1999', DoubleType(), True),\n",
    "        StructField('2000', DoubleType(), True),\n",
    "        StructField('2001', DoubleType(), True),\n",
    "        StructField('2002', DoubleType(), True),\n",
    "        StructField('2003', DoubleType(), True),\n",
    "        StructField('2004', DoubleType(), True),\n",
    "        StructField('2005', DoubleType(), True),\n",
    "        StructField('2006', DoubleType(), True),\n",
    "        StructField('2007', DoubleType(), True),\n",
    "        StructField('2008', DoubleType(), True),\n",
    "        StructField('2009', DoubleType(), True),\n",
    "        StructField('2010', DoubleType(), True),\n",
    "        StructField('2011', DoubleType(), True),\n",
    "        StructField('2012', DoubleType(), True),\n",
    "        StructField('2013', DoubleType(), True),\n",
    "        StructField('2014', DoubleType(), True),\n",
    "        StructField('2015', DoubleType(), True),\n",
    "        StructField('2016', DoubleType(), True),\n",
    "        StructField('2017', DoubleType(), True  ),\n",
    "        StructField('2018', DoubleType(), True),\n",
    "        StructField('2019', DoubleType(), True),\n",
    "        StructField('2020', DoubleType(), True),\n",
    "        StructField('2021', DoubleType(), True),\n",
    "        StructField('2022', DoubleType(), True),\n",
    "        StructField('2023', DoubleType(), True),\n",
    "        StructField('2024', DoubleType(), True),\n",
    "        StructField('2025', DoubleType(), True),\n",
    "        StructField('2026', DoubleType(), True),\n",
    "        StructField('2027', DoubleType(), True),\n",
    "        StructField('2028', DoubleType(), True),\n",
    "        StructField('2029', DoubleType(), True),\n",
    "        StructField('2030', DoubleType(), True),\n",
    "        StructField('2031', DoubleType(), True),\n",
    "        StructField('2032', DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926903dc-79bb-4869-b034-c636aca27bf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Explicitly define schema for gold level table\n",
    "\n",
    "gold_ids_schema = StructType([\n",
    "        StructField('country_name', StringType(), True),\n",
    "        StructField('country_code', StringType(), True),\n",
    "        StructField('counterpart_area_name', StringType(), True),\n",
    "        StructField('counterpart_area_code', StringType(), True),\n",
    "        StructField('series_name', StringType(), True),\n",
    "        StructField('series_code', StringType(), True),\n",
    "        StructField('2021', DoubleType(), True),\n",
    "        StructField('2022', DoubleType(), True),\n",
    "        StructField('2023', DoubleType(), True),\n",
    "        StructField('2024', DoubleType(), True),\n",
    "        StructField('2025', DoubleType(), True),\n",
    "        StructField('2026', DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54c12e15-77a4-4cef-a5ce-b0d8998f1010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check if bronze level table exists; create if it does not\n",
    "\n",
    "def check_or_create_bronze_ids_table(bronze_schema, logger):\n",
    "\n",
    "    logger.info(f'Checking to see if bronze table exists')\n",
    "\n",
    "    bronze_ids_table = spark.createDataFrame(data=[], schema=bronze_schema)\n",
    "    ids_table_name = 'bronze_ids_table'\n",
    "\n",
    "    bronze_ids_table.write.format('delta').mode('ignore').saveAsTable(f'{ids_table_name}')\n",
    "\n",
    "    logger.info(f'Created {ids_table_name} if not exists')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f8315ed-561c-4f1c-947e-b8d7630081b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check if silver level table exists; create if it does not\n",
    "\n",
    "def check_or_create_silver_ids_table(silver_schema, logger):\n",
    "\n",
    "    logger.info(f'Checking to see if silver table exists')\n",
    "\n",
    "    silver_ids_table = spark.createDataFrame(data=[], schema = silver_schema)\n",
    "    ids_table_name = 'silver_ids_table'\n",
    "\n",
    "    silver_ids_table.write.format('delta').mode('ignore').saveAsTable(f'{ids_table_name}')\n",
    "\n",
    "    logger.info(f'Created {ids_table_name} if not exists')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2bf7c3e-52ae-4124-9eb7-ec56742bba67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Check if gold level table exists; create if it does not\n",
    "\n",
    "def check_or_create_gold_ids_table(gold_schema, logger):\n",
    "\n",
    "    logger.info(f'Checking to see if gold table exists')\n",
    "\n",
    "    gold_ids_table = spark.createDataFrame(data=[], schema = gold_schema)\n",
    "    ids_table_name = 'gold_ids_table'\n",
    "\n",
    "    gold_ids_table.write.format('delta').mode('ignore').saveAsTable(f'{ids_table_name}')\n",
    "\n",
    "    logger.info(f'Created {ids_table_name} if not exists')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ddd45f-7a1e-467b-840e-4a9c2a6c0d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Setup basic logging function for tracking and troubleshooting\n",
    "\n",
    "def setup_logging():\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(f'logs/etl_run_{datetime.now().strftime('%Y%m%d')}.log'),\n",
    "            logging.StreamHandler(sys.stdout)\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009d042e-6b11-4d55-8344-1b0d130852e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Function to identify correct files to load into bronze level table\n",
    "\n",
    "def get_correct_files(directory_path, volume, logger):\n",
    "    files_count = 0\n",
    "    for dirpath,_,filenames in os.walk(directory_path):\n",
    "        for file in filenames:\n",
    "            if 'IDS_ALL' in file:\n",
    "                files_count += 1\n",
    "                logger.info(f'{file} is in the directory')\n",
    "                volume.append(os.path.join(dirpath, file))\n",
    "            else:\n",
    "                pass\n",
    "    logger.info(f'{files_count} files to load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a47499-8675-4d80-813e-0a6b303143a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Function to read a csv data file for the bronze level table\n",
    "\n",
    "def ids_data_extractor(spark, input_path, schema, logger):\n",
    "\n",
    "    logger.info(f'Extracting data from {input_path}')\n",
    "\n",
    "    df = spark.read.csv(input_path,\n",
    "                        header = True,\n",
    "                         schema = schema)\n",
    "    total_row_count = df.count()\n",
    "    total_column_count = len(df.columns)\n",
    "\n",
    "    logger.info(f'Extracted {total_row_count} rows with {total_column_count} columns')\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46880a46-22bd-41cc-94b7-468978c2a849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Function to write/append data to bronze level table\n",
    "\n",
    "def write_to_tbl(df, output_path, logger):\n",
    "    logger.info(f'Writing data to {output_path}')\n",
    "\n",
    "    df.write.format('delta').mode('append').saveAsTable(output_path)\n",
    "\n",
    "    total_row_count = df.count()\n",
    "\n",
    "    logger.info(f'Wrote {total_row_count} rows to {output_path}')\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d93a3153-9dbc-4589-8b84-0b9e82cfcba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Function that uses regex to translate non-english characters to standard english characters; for the silver level table\n",
    "\n",
    "def translate_character_to_english(df, logger):\n",
    "    non_english_chars = 'ãäöüẞáäčďéěíĺľňóôŕšťúůýžÄÖÜẞÁÄČĎÉĚÍĹĽŇÓÔŔŠŤÚŮÝŽ'\n",
    "    english_chars = 'aaousaacdeeillnoorstuuyzAOUSAACDEEILLNOORSTUUYZ'\n",
    "\n",
    "    logger.info(f'Eliminating non-english characters from string fields')\n",
    "\n",
    "    df = df.withColumn('country_name', F.translate('country_name',non_english_chars,english_chars))\n",
    "    df = df.withColumn('country_code', F.translate('country_code',non_english_chars,english_chars))   \n",
    "    df = df.withColumn('counterpart_area_name', F.translate('counterpart_area_name',non_english_chars,english_chars))\n",
    "    df = df.withColumn('counterpart_area_code', F.translate('counterpart_area_code',non_english_chars,english_chars))\n",
    "    df = df.withColumn('series_name', F.translate('series_name',non_english_chars,english_chars))\n",
    "    df = df.withColumn('series_code', F.translate('series_code',non_english_chars,english_chars))\n",
    "\n",
    "    logger.info(f'Elimination complete')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be349180-3b9c-44a7-a482-fc9f2c1cf1b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Function to drop rows that do not relate to a usable metric; for silver table\n",
    "\n",
    "def drop_invalid_rows(df, logger):\n",
    "\n",
    "    logger.info(f'Dropping rows with no country_code')\n",
    "\n",
    "    df = df.dropna(subset =['country_code'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e88524d3-c4c2-48ed-a193-4e98e2c4c53d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Function to truncate amount of data to the desired timeframe (2021-2024); for silver table\n",
    "\n",
    "def drop_excess_columns(df, logger):\n",
    "\n",
    "    logger.info(f'Dropping excess year fields')\n",
    "                \n",
    "    df = df.drop('1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020','2025','2026','2027', '2028', '2029', '2030', '2031', '2032')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41bb1680-ffe1-48b2-aa75-13df5c825c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Function to average percent values for the currency composition metric; for the gold level table\n",
    "def process_gold_tbl(logger):\n",
    "\n",
    "    logger.info(f'Creating table for currency composition analyst')\n",
    "\n",
    "    df =  spark.sql('''SELECT * FROM international_debt_statistics_data.default.ids_silver_tbl\n",
    "                    WHERE series_name like 'Currency composition%' ''')\n",
    "    \n",
    "    logger.info(f'Averaging values from 2021 to 2024')\n",
    "\n",
    "    df = df.withColumn('2021_to_2024_average_percent', (df['2021'] + df['2022'] + df['2023'] + df['2024']))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1546971-592c-4fb0-92f0-2a2882e15c72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Main function houses all intermediary functions(ETLs, reads, writes, etc...) and variables used by intermediary functions.\n",
    "#Raises an exception if a step fails in the pipeline.\n",
    "#Logs each step in the pipeline\n",
    "\n",
    "#Produces a gold table for the average currency composition metric across (2021-2024)\n",
    "#Produces a silver table as a clean table for all metrics\n",
    "#Produces a bronze table for reading data\n",
    "\n",
    "def main():\n",
    "\n",
    "    logger = setup_logging()\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    os.makedirs('data/processsed/batches', exist_ok=True)\n",
    "\n",
    "    directory_path = '/Volumes/international_debt_statistics_data/default/ids_data'\n",
    "    ids_data_volume_files = []\n",
    "\n",
    "    output_bronze_table = 'international_debt_statistics_data.default.ids_bronze_tbl'\n",
    "    output_silver_table = 'international_debt_statistics_data.default.ids_silver_tbl'\n",
    "    output_gold_table = 'international_debt_statistics_data.default.ids_gold_tbl'\n",
    "\n",
    "    check_or_create_bronze_ids_table(bronze_ids_schema, logger)\n",
    "    check_or_create_silver_ids_table(silver_ids_schema, logger)\n",
    "    check_or_create_gold_ids_table(gold_ids_schema, logger)\n",
    "\n",
    "    logger.info(f'Starting IDS ETL Pipeline')\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    get_correct_files(directory_path, ids_data_volume_files, logger)\n",
    "\n",
    "    for files in ids_data_volume_files:\n",
    "        input_path = files\n",
    "        try:\n",
    "\n",
    "            bronze_df = ids_data_extractor(spark, input_path, bronze_ids_schema, logger)\n",
    "            write_to_tbl(bronze_df, output_bronze_table, logger)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Pipeline failed: {str(e)}')\n",
    "            logger.error(traceback.format_exc())\n",
    "            raise\n",
    "\n",
    "    try:\n",
    "        bronze_df = spark.sql('''SELECT * FROM international_debt_statistics_data.default.ids_bronze_tbl''')\n",
    "        silver_df = translate_character_to_english(bronze_df, logger)\n",
    "        silver_df = drop_excess_columns(silver_df, logger)\n",
    "        silver_df = drop_invalid_rows(silver_df, logger)\n",
    "        write_to_tbl(silver_df, output_silver_table, logger)\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f'Pipeline failed: {str(e)}')\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "\n",
    "        gold_df = process_gold_tbl(logger)\n",
    "        write_to_tbl(gold_df, output_gold_table, logger)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f'Pipeline failed: {str(e)}')\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise      \n",
    "\n",
    "    finally:\n",
    "        logger.info('Spark session closed.')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4567407102809257,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "IDS_Data Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
